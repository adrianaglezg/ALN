{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Métodos iterativos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Hasta ahora sólo hemos visto métodos de solución directa. La característica común de estos métodos es que calculan la solución con un número finito de operaciones. Además, si el ordenador tuviera una precisión infinita (sin errores de redondeo), la solución sería exacta.\n",
    "Los métodos iterativos, o indirectos, comienzan con una estimación inicial de la solución $\\mathbf{x}$ y van mejorándola progresivamente hasta que el cambio en $\\mathbf{x}$ es insignificante. Dado que el número de iteraciones necesario puede ser elevado, los métodos indirectos son, en general, más lentos que los directos. Sin embargo, los métodos iterativos tienen las siguientes dos ventajas que los hacen convenientes para ciertos problemas:\n",
    "\n",
    "1. Es posible almacenar sólo los elementos no nulos de la matriz de coeficientes. Esto permite tratar con matrices muy grandes que sean dispersas (y no necesariamente por bandas). En muchos problemas, no es ni siquiera necesario almacenar la matriz de coeficientes.\n",
    "2. Los procedimientos iterativos se *autocorrigen*. Esto significa que los errores de redondeo (o incluso los errores aritméticos) en una determinada iteración se corrigen en las iteraciones subsiguientes.\n",
    "\n",
    "Un grave inconveniente de los métodos iterativos es que no siempre convergen a la solución. Se puede demostrar que la convergencia está garantizada solo si la matriz de coeficientes es diagonal dominante. El valor inicial de $\\mathbf{x}$ no determina en ningún caso si el algoritmo convergerá o no: si el método converge para un vector inicial, lo hará para cualquiera. El valor inicial sólo afecta al número de iteraciones necesarias para la convergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método Gauss-Seidel\n",
    "\n",
    "El sistema de ecuaciones $\\mathbf{Ax} = \\mathbf{b}$ puede escribirse en notación escalar como:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^n A_{i j} x_j=b_i, \\quad i=1,2, \\ldots, n\n",
    "$$\n",
    "\n",
    "Extrayendo el término que contiene $x_i$ del sumatorio se obtiene\n",
    "\n",
    "$$\n",
    "A_{i i} x_i+\\sum_{\\substack{j=1 \\\\ j \\neq i}}^n A_{i j} x_j=b_i, \\quad i=1,2, \\ldots, n\n",
    "$$\n",
    "\n",
    "Resolviendo para $x_i$ obtenemos\n",
    "\n",
    "$$\n",
    "x_i=\\frac{1}{A_{i i}}\\left(b_i-\\sum_{\\substack{j=1 \\\\ j \\neq i}}^n A_{i j} x_j\\right), \\quad i=1,2, \\ldots, n\n",
    "$$\n",
    "\n",
    "Esta última ecuación sugiere el siguiente esquema iterativo:\n",
    "$$\n",
    "x_i \\leftarrow \\frac{1}{A_{i i}}\\left(b_i-\\sum_{\\substack{j=1 \\\\ j \\neq i}}^n A_{i j} x_j\\right), \\quad i=1,2, \\ldots, n\n",
    "$$\n",
    " \n",
    "Comenzamos eligiendo el vector inicial $\\mathbf{x}$. Si no se dispone de una buena estimación de la solución, $\\mathbf{x}$ puede elegirse al azar. A continuación, se utiliza la ecuación anterior para volver a calcular cada elemento de $\\mathbf{x}$, utilizando siempre los últimos valores disponibles de $x_j$. El procedimiento se repite hasta que los cambios en $\\mathbf{x}$ entre las sucesivas iteraciones sean lo suficientemente pequeños.\n",
    "\n",
    "La convergencia del método Gauss-Seidel puede mejorarse mediante una técnica conocida como *relajación*. La idea es tomar el nuevo valor de $x_i$ como una media ponderada de su valor anterior y el nuevo. La fórmula iterativa correspondiente es\n",
    "\n",
    "$$\n",
    "x_i \\leftarrow \\frac{\\omega}{A_{i i}}\\left(b_i-\\sum_{\\substack{j=1 \\\\ j \\neq i}}^n A_{i j} x_j\\right)+(1-\\omega) x_i, \\quad i=1,2, \\ldots, n\n",
    "$$ \n",
    "\n",
    "donde el peso $\\omega$ se llama factor de relajación. Se puede ver que si $\\omega = 1$, no se produce ninguna relajación. Si $\\omega < 1$, la ecuación anterior representa una interpolación entre la antigua $x_i$ y la nueva. Esto se denomina *infra-relajación*. En los casos en los que $\\omega > 1$, tenemos extrapolación, o *sobrerrelajación*.\n",
    "\n",
    "No existe un método inequívoco para determinar de antemano el valor óptimo de $\\omega$, lo que suele hacerse es calcular una estimación al inicio del proceso iterativo. Sea $\\Delta x^{(k)} = \\left| \\mathbf{x}^{(k-1)} - \\mathbf{x}^{(k)} \\right|$ la magnitud del cambio en $\\mathbf{x}$ durante la $k$-ésima iteración (realizada sin relajación, es decir, con $\\omega = 1$). Si $k$ es suficientemente grande (se suele tomar $k \\ge 5$), se puede demostrar que una aproximación del valor óptimo de $\\omega$ es\n",
    "\n",
    "$$\n",
    "\\omega_{\\mathrm{opt}} \\approx \\frac{2}{1+\\sqrt{1-\\left(\\Delta x^{(k+p)} / \\Delta x^{(k)}\\right)^{1 / p}}}\n",
    "$$\n",
    " \n",
    "donde $p$ es un número entero positivo.\n",
    "\n",
    "De esta forma, el procedimiento habitual con el algoritmo Gauss-Seidel con relajación es el siguiente:\n",
    "1. Calcula $k$ iteraciones con $\\omega=1$ ($k=10$ es razonable).\n",
    "2. Almacena $\\Delta x^{(k)}$.\n",
    "3. Realiza $p$ iteraciones más.\n",
    "4. Almacena $\\Delta x^{(k+p)}$.\n",
    "5. Calcula $\\omega_{\\text {opt }}$.\n",
    "6. Realiza todas las siguientes iteraciones con $\\omega=\\omega_{\\text {opt }}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 1 -** Programa una función ```gauss_seidel``` que implemente el método Gauss-Seidel con relajación. Calcula automáticamente $\\omega_{opt}$ según se ha explicado anteriormente utilizando $k = 10$ y $p = 1$. La función debe devolver el vector solución, el número de iteraciones realizadas y el valor de $\\omega_{opt}$ utilizado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iter_esq(A, x, b, omega):\n",
    "\n",
    "    n = len(b)\n",
    "    x_old = np.copy(x)\n",
    "\n",
    "    for i in range(n):\n",
    "        x[i] = b[i] \n",
    "\n",
    "        for j in range(n):\n",
    "            if j == i:\n",
    "                continue # si i = j se salta esta iteracion\n",
    "            x[i] -= A[i, j] * x[j]\n",
    "\n",
    "        x[i] *= omega / A[i, i]\n",
    "\n",
    "        x[i] += (1 - omega) * x_old[i]\n",
    "        \n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def solve_gauss_seidel(\n",
    "        A: np.ndarray,\n",
    "        x: np.ndarray,\n",
    "        b: np.ndarray,\n",
    "        omega: float,\n",
    "        tol: float,\n",
    "        max_n_ite: int) -> np.ndarray :\n",
    "    \n",
    "    for i in range(1, max_n_ite):\n",
    "\n",
    "        # guardo el candidato actual para comparar\n",
    "        x_old = np.copy(x)\n",
    "\n",
    "        # aaplicar el sistema iterativo de gauss seidel\n",
    "\n",
    "        x = iter_esq(A, x, b, omega)\n",
    "\n",
    "        #comprobar lo de la tolerancia\n",
    "\n",
    "        dx = np.linalg.norm(x - x_old) # normal euclidea\n",
    "        \n",
    "        error = dx / np.linalg.norm(x)\n",
    "\n",
    "\n",
    "        if error < tol:\n",
    "\n",
    "            return x, i, omega\n",
    "        \n",
    "    print(f\" cuidadillo: la tolerancia objetivo no ha sido alcanzada. la solucion actual tiene un error de {error}\")\n",
    "\n",
    "    return x, i, omega\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.99999691, -2.00000353,  2.99999866]), 27, 0.7)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[4.0, -2.0, 1.0],\n",
    "             [-2.0, 4.0, -2.0],\n",
    "             [1.0, -2.0, 4.0]], dtype = float)\n",
    "\n",
    "b = np.array([11, -16, 17])\n",
    "\n",
    "x = np.array([0.0, 0.0, 0.0])\n",
    "omega = 0.7\n",
    "tol = 10 ** -6\n",
    "max_ite = 1000\n",
    "\n",
    "solution = solve_gauss_seidel(A, x, b, omega, tol, max_ite)\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def iter_esq(A, x, b, omega):\n",
    "\n",
    "    n = len(b)\n",
    "    x_old = np.copy(x)\n",
    "\n",
    "    for i in range(n):\n",
    "        x[i] = b[i] \n",
    "\n",
    "        for j in range(n):\n",
    "            if j == i:\n",
    "                continue # si i = j se salta esta iteracion\n",
    "            x[i] -= A[i, j] * x[j]\n",
    "\n",
    "        x[i] *= omega / A[i, i]\n",
    "\n",
    "        x[i] += (1 - omega) * x_old[i]\n",
    "\n",
    "    return x\n",
    "\n",
    "def solve_gauss_seidel_relax(\n",
    "        A: np.ndarray,\n",
    "        x: np.ndarray,\n",
    "        b: np.ndarray,\n",
    "        tol: float,\n",
    "        max_n_ite: int) -> np.ndarray :\n",
    "    \n",
    "    omega = 1\n",
    "    k = 10\n",
    "    p = 1\n",
    "\n",
    "    for i in range(1, max_n_ite):\n",
    "\n",
    "        # guardo el candidato actual para comparar\n",
    "        x_old = np.copy(x)\n",
    "\n",
    "        # aaplicar el sistema iterativo de gauss seidel\n",
    "\n",
    "        x = iter_esq(A, x, b, omega)\n",
    "\n",
    "        #comprobar lo de la tolerancia\n",
    "\n",
    "        dx = np.linalg.norm(x - x_old) # normal euclidea\n",
    "        \n",
    "        error = dx / np.linalg.norm(x)\n",
    "\n",
    "\n",
    "        if error < tol:\n",
    "\n",
    "            return x, i, omega\n",
    "        \n",
    "        if i == k:\n",
    "            dx1 = dx\n",
    "\n",
    "\n",
    "        if i == (k+p):\n",
    "            dx2 = dx\n",
    "            omega = 2 / (1 + (1 - (dx2 / dx1) ** (1 / p)) ** 0.5)\n",
    "        \n",
    "    print(f\" cuidadillo: la tolerancia objetivo no ha sido alcanzada. la solucion actual tiene un error de {error}\")\n",
    "\n",
    "    return x, i, omega\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.99999998, -1.99999994,  3.00000003]),\n",
       " 14,\n",
       " np.float64(1.0835680928076241))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([[4.0, -2.0, 1.0],\n",
    "             [-2.0, 4.0, -2.0],\n",
    "             [1.0, -2.0, 4.0]], dtype = float)\n",
    "\n",
    "b = np.array([11, -16, 17])\n",
    "\n",
    "x = np.array([0.0, 0.0, 0.0])\n",
    "tol = 10 ** -6\n",
    "max_ite = 1000\n",
    "\n",
    "solution = solve_gauss_seidel_relax(A, x, b, tol, max_ite)\n",
    "solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# implementacion para sistemas concretos\n",
    "\n",
    "#enunciado problema 27 de la hoja\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def iter_esq(A, x, b, omega):\n",
    "\n",
    "    n = len(b)\n",
    "    x_old = np.copy(x)\n",
    "\n",
    "\n",
    "    x[0] = omega * (x[1] - x[n-1]) / 4.0 + (1.0 - omega) * x[0]\n",
    "\n",
    "    for i in range(1, n-1): # desde 0 no porue ya lo dfines a parte\n",
    "        x[i] = omega * (x[i-1] x[i+1]) / 4.0 + (1.0 - omega) * x[0]\n",
    "    \n",
    "    x[n - 1] = omega * (100 - x[0] - x[n-2]) / 4.0 + (1.0 - omega) * x[n-1]\n",
    "\n",
    "    return x\n",
    "\n",
    "def solve_gauss_seidel_relax_sistema_J(\n",
    "        # A: np.ndarray,\n",
    "        x: np.ndarray,\n",
    "        b: np.ndarray,\n",
    "        tol: float,\n",
    "        max_n_ite: int) -> np.ndarray :\n",
    "    \n",
    "    omega = 1\n",
    "    k = 10\n",
    "    p = 1\n",
    "\n",
    "    for i in range(1, max_n_ite):\n",
    "\n",
    "        # guardo el candidato actual para comparar\n",
    "        x_old = np.copy(x)\n",
    "\n",
    "        # aaplicar el sistema iterativo de gauss seidel\n",
    "\n",
    "        x = iter_esq(A, x, b, omega)\n",
    "\n",
    "        #comprobar lo de la tolerancia\n",
    "\n",
    "        dx = np.linalg.norm(x - x_old) # normal euclidea\n",
    "        \n",
    "        error = dx / np.linalg.norm(x)\n",
    "\n",
    "\n",
    "        if error < tol:\n",
    "\n",
    "            return x, i, omega\n",
    "        \n",
    "        if i == k:\n",
    "            dx1 = dx\n",
    "\n",
    "\n",
    "        if i == (k+p):\n",
    "            dx2 = dx\n",
    "            omega = 2 / (1 + (1 - (dx2 / dx1) ** (1 / p)) ** 0.5)\n",
    "        \n",
    "    print(f\" cuidadillo: la tolerancia objetivo no ha sido alcanzada. la solucion actual tiene un error de {error}\")\n",
    "\n",
    "    return x, i, omega\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "n = 100\n",
    "\n",
    "x = np.zeros(10)\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2 -** Resuelve el siguiente sistema de ecuaciones \n",
    "$$\n",
    "\\left[\\begin{array}{rrr}\n",
    "4 & -1 & 1 \\\\\n",
    "-1 & 4 & -2 \\\\\n",
    "1 & -2 & 4\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
    "12 \\\\\n",
    "-1 \\\\\n",
    "5\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "mediante el método de Gauss-Seidel sin relajación, operando a mano.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3 -** Escribe un programa para resolver el siguiente sistema de $n$ ecuaciones por el método de Gauss-Seidel con relajación (el programa debe funcionar con cualquier valor de $n$):\n",
    "$$\n",
    "\\left[\\begin{array}{rrrrrrrrr}\n",
    "2 & -1 & 0 & 0 & \\ldots & 0 & 0 & 0 & 1 \\\\\n",
    "-1 & 2 & -1 & 0 & \\ldots & 0 & 0 & 0 & 0 \\\\\n",
    "0 & -1 & 2 & -1 & \\ldots & 0 & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\ldots & -1 & 2 & -1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\ldots & 0 & -1 & 2 & -1 \\\\\n",
    "1 & 0 & 0 & 0 & \\ldots & 0 & 0 & -1 & 2\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{n-2} \\\\\n",
    "x_{n-1} \\\\\n",
    "x_n\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Puedes reutilizar código anterior (Ejercicio 1). Ejecuta el programa con $n = 20$. Se puede demostrar que la solución exacta es $x_i = -n/4 +i/2, i = 1, 2, \\dots , n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Método del gradiente conjugado\n",
    "\n",
    "Consideremos el problema de encontrar el vector $\\mathbf{x}$ que minimiza la función escalar siguiente:\n",
    "\n",
    "$$\n",
    "f(\\mathbf{x})=\\frac{1}{2} \\mathbf{x}^T \\mathbf{A} \\mathbf{x}-\\mathbf{b}^T \\mathbf{x}\n",
    "$$\n",
    "\n",
    "donde la matriz $\\mathbf{A}$ es simétrica y definida positiva. Como $f(\\mathbf{x})$ se minimiza cuando su gradiente $\\nabla f = \\mathbf{A} \\mathbf{x}-\\mathbf{b}$ es cero, vemos que esta minimización es equivalente a resolver:\n",
    "\n",
    "$$\n",
    "\\mathbf{A} \\mathbf{x} = \\mathbf{b}\n",
    "$$\n",
    "\n",
    "Los métodos del gradiente logran esta minimización iterando desde un vector inicial $x_0$. En cada iteración $k$ se calcula una solución refinada\n",
    "\n",
    "$$\n",
    "\\mathbf{x}_{k+1}=\\mathbf{x}_k+\\alpha_k \\mathbf{s}_k\n",
    "$$\n",
    "\n",
    "\n",
    "La *longitud de paso* $\\alpha_k$ se elige de forma que $\\mathbf{x}_{k+1}$ minimice $f(\\mathbf{x}_{k+1})$ en la *dirección de búsqueda* $\\mathbf{s}_k$. Es decir, $\\mathbf{x}_{k+1}$ debe satisfacer el sistema de ecuaciones a resolver:\n",
    "$$\n",
    "\\mathbf{A}\\left(\\mathbf{x}_k+\\alpha_k \\mathbf{s}_k\\right)=\\mathbf{b}\n",
    "$$\n",
    "\n",
    "Ahora, introducimos el concepto de *residuo*\n",
    "\n",
    "$$\n",
    "\\mathbf{r}_k=\\mathbf{b}-\\mathbf{A x}_k\n",
    "$$\n",
    "por lo que la ecuación anterior se puede poner como $\\alpha \\mathbf{A} \\mathbf{s}_k=\\mathbf{r}_k$. Pre-multiplicando ambos lados por $\\mathbf{s}_k^T$ y resolviendo para $\\alpha_k$, tenemos\n",
    "$$\n",
    "\\alpha_k=\\frac{\\mathbf{s}_k^T \\mathbf{r}_k}{\\mathbf{s}_k^T \\mathbf{A} \\mathbf{s}_k}\n",
    "$$\n",
    " \n",
    "Todavía nos queda el problema de determinar la dirección de búsqueda $\\mathbf{s}_k$. La intuición nos dice que elijamos $\\mathbf{s}_k=-\\nabla f=\\mathbf{r}_k$, porque esta es la dirección del mayor cambio negativo en $f(x)$. El procedimiento resultante se conoce como el *método del descenso más pronunciado*. No es un algoritmo popular porque su convergencia puede ser lenta. El método del gradiente conjugado, más eficiente, utiliza la dirección de búsqueda\n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{k+1}=\\mathbf{r}_{k+1}+\\beta_k \\mathbf{s}_k\n",
    "$$\n",
    "\n",
    "La constante $\\beta_k$ se elige para que las dos direcciones de búsqueda sucesivas sean conjugadas\n",
    "entre sí, lo que significa que \n",
    "\n",
    "$$\n",
    "\\mathbf{s}_{k+1}^T \\mathbf{A} \\mathbf{s}_k=0\n",
    "$$\n",
    " \n",
    "El gran atractivo de los gradientes conjugados es que la minimización en una dirección conjugada no va en contra de las minimizaciones anteriores (las minimizaciones no interfieren entre sí).\n",
    "Usando este $s_{k+1}$ se obtiene que\n",
    " \n",
    "$$\n",
    "\\left(\\mathbf{r}_{k+1}^T+\\beta_k \\mathbf{s}_k^T\\right) \\mathbf{A} \\mathbf{s}_k=0\n",
    "$$\n",
    "\n",
    "que da como resultado\n",
    " \n",
    "$$\n",
    "\\beta_k=-\\frac{\\mathbf{r}_{k+1}^T \\mathbf{A} \\mathbf{s}_k}{\\mathbf{s}_k^T \\mathbf{A} \\mathbf{s}_k}\n",
    "$$ \n",
    "\n",
    "A continuación se muestra el pseudocódigo del algoritmo de gradiente conjugado:\n",
    "\n",
    "1. Elegir $\\mathbf{x}_0$ (arbitrariamente).\n",
    "2. Sea $\\mathbf{r}_0 \\leftarrow \\mathbf{b}-\\mathbf{A x}_0$.\n",
    "3. Sea $\\mathbf{s}_0 \\leftarrow \\mathbf{r}_0$ (si se carece de una dirección de búsqueda previa, elija la dirección de gradiente más pronunciada).\n",
    "4. hacer para $k=0,1,2, \\ldots$ :\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\alpha_k \\leftarrow \\frac{\\mathbf{s}_k^T \\mathbf{r}_k}{\\mathbf{s}_k^T \\mathbf{A} \\mathbf{s}_k} \\text {. } \\\\\n",
    "&\\mathbf{x}_{k+1} \\leftarrow \\mathbf{x}_k+\\alpha_k \\mathbf{s}_k . \\\\\n",
    "&\\mathbf{r}_{k+1} \\leftarrow \\mathbf{b}-\\mathbf{A x}_{k+1} \\text {. } \\\\\n",
    "&\\text { si }\\left|\\mathbf{r}_{k+1}\\right| \\leq \\varepsilon \\text { exit loop (} \\varepsilon \\text { es la tolerancia del error). } \\\\\n",
    "&\\beta_k \\leftarrow-\\frac{\\mathbf{r}_{k+1}^T \\mathbf{A} \\mathbf{s}_k}{\\mathbf{s}_k^T \\mathbf{A} \\mathbf{s}_k} \\text {. } \\\\\n",
    "&\\mathbf{s}_{k+1} \\leftarrow \\mathbf{r}_{k+1}+\\beta_k \\mathbf{s}_k . \\\\\n",
    "&\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "Se puede demostrar que los vectores residuo $\\mathbf{r}_1, \\mathbf{r}_2, \\mathbf{r}_3, \\dots$ producidos por el algoritmo son mutuamente ortogonales; es decir, $\\mathbf{r}_i \\cdot \\mathbf{r}_j=0, i \\neq j$. Supongamos ahora que hemos realizado suficientes iteraciones hasta haber calculado $n$ vectores residuo (recuerda que $n$ es la dimensión del problema). Ocurre entonces que el residuo resultante de la siguiente iteración será un vector nulo ($\\mathbf{r}_{n+1} = \\mathbf{0}$), lo que indica que se ha obtenido la solución exacta. Por lo tanto, parece que el algoritmo del gradiente conjugado\n",
    "no es un método iterativo, porque alcanza efectivamente la solución después de $n$ iteraciones. Lo que suele ocurrir en la práctica es que la convergencia se alcanza en menos de $n$ iteraciones.\n",
    "\n",
    "El método del gradiente conjugado no puede competir con los métodos directos en la resolución de pequeños sistemas de ecuaciones. Su fuerza reside en el manejo de sistemas grandes y dispersos (donde la mayoría de los elementos de $\\mathbf{A}$ son cero). Es importante señalar que $\\mathbf{A}$ sólo aparece en el algoritmo a través de su multiplicación por un vector; es decir, en la forma $\\mathbf{Av}$, donde $\\mathbf{v}$ es un vector (ya sea $\\mathbf{s}_{k+1}$ o $\\mathbf{s}_{k}$ ). Si $\\mathbf{A}$ es dispersa, es posible escribir una función eficiente para la multiplicación de $\\mathbf{A}$ por un vector y pasarla, en lugar de $A$, al algoritmo del gradiente conjugado."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 4 -** Escribe una función ```conj_grad``` que implemente el algoritmo del gradiente conjugado. El número máximo de iteraciones permitido se fija en $n$ (el número de incógnitas). Dicha función debe internamente llamar a otra llamada ```av``` que devuelve el producto $\\mathbf{Av}$. Esta función debe ser aportada por el usuario. El usuario también debe aportada el vector inicial $\\mathbf{x}_0$ y el vector de términos independientes $\\mathbf{b}$. La función devuelve el vector solución $\\mathbf{x}$ y el número de iteraciones realizadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 5 -** Resuelve el siguiente sistema de ecuaciones \n",
    "$$\n",
    "\\left[\\begin{array}{rrr}\n",
    "4 & -1 & 1 \\\\\n",
    "-1 & 4 & -2 \\\\\n",
    "1 & -2 & 4\n",
    "\\end{array}\\right]\\left[\\begin{array}{l}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3\n",
    "\\end{array}\\right]=\\left[\\begin{array}{r}\n",
    "12 \\\\\n",
    "-1 \\\\\n",
    "5\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "mediante el método del gradiente conjugado, operando a mano."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 6 -** Escribe un programa de ordenador para resolver el siguiente sistema de $n$ ecuaciones por el método del gradiente conjugado (el programa debe funcionar con cualquier valor de $n$):\n",
    "$$\n",
    "\\left[\\begin{array}{rrrrrrrrr}\n",
    "2 & -1 & 0 & 0 & \\ldots & 0 & 0 & 0 & 1 \\\\\n",
    "-1 & 2 & -1 & 0 & \\ldots & 0 & 0 & 0 & 0 \\\\\n",
    "0 & -1 & 2 & -1 & \\ldots & 0 & 0 & 0 & 0 \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\vdots & & \\vdots & \\vdots & \\vdots & \\vdots \\\\\n",
    "0 & 0 & 0 & 0 & \\ldots & -1 & 2 & -1 & 0 \\\\\n",
    "0 & 0 & 0 & 0 & \\ldots & 0 & -1 & 2 & -1 \\\\\n",
    "1 & 0 & 0 & 0 & \\ldots & 0 & 0 & -1 & 2\n",
    "\\end{array}\\right]\\left[\\begin{array}{c}\n",
    "x_1 \\\\\n",
    "x_2 \\\\\n",
    "x_3 \\\\\n",
    "\\vdots \\\\\n",
    "x_{n-2} \\\\\n",
    "x_{n-1} \\\\\n",
    "x_n\n",
    "\\end{array}\\right]=\\left[\\begin{array}{c}\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "\\vdots \\\\\n",
    "0 \\\\\n",
    "0 \\\\\n",
    "1\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "Puedes reutilizar código anterior (Ejercicio 4). Ejecuta el programa con $n = 20$. Se puede demostrar que la solución exacta es $x_i = -n/4 +i/2, i = 1, 2, \\dots , n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
