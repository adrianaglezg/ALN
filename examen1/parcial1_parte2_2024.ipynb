{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-KaZACAAOrWG"
      },
      "source": [
        "# Primer parcial - Algorítmica numérica"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aODTfjF5OrWH"
      },
      "source": [
        "## Segunda Parte: **Preguntas de desarrollo** - con apuntes, 60min, total 5 puntos\n",
        "\n",
        "Consideraciones generales: Para que otorguen puntos, **todas las respuestas deben estar justificadas**, ya sea en texto, escribiendo \"a mano\" o en comentarios en el código."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6dJz6NjOrWI"
      },
      "source": [
        "## Problema 1 (2.2 puntos)\n",
        "\n",
        "Euler descubrió la siguiente fórmula cuadrática:\n",
        "\n",
        "$$ n^2 + n + 41 $$\n",
        "\n",
        "Resulta que la fórmula produce 40 números primos cuando se evalúa con cualquier entero entre  $0$ y $39$. Sin embargo, cuando $n = 40$, $40^2 + 40 + 41 = 40(40 + 1) + 41$ es divisible por 41, y ciertamente cuando $n = 41$, $41^2 + 41 + 41$ es claramente divisible por 41.\n",
        "\n",
        "Más tarde, se descubrió una fórmula similar: $n^2 - 79n + 1601$, que produce 80 números primos para los enteros consecutivos $0 \\leq n \\leq 79$. Obsérvese que el producto de los coeficientes, -79 y 1601, es -126479.\n",
        "\n",
        "Consideremos ahora una fórmula cuadrática genérica de la forma:\n",
        "\n",
        "$$ n^2 + an + b, $$\n",
        "\n",
        "donde $|a| < m$ y $|b| \\leq m$. El valor $m$ se fija más abajo.\n",
        "\n",
        "Encuentra el producto de los coeficientes $a$ y $b$ que produce el máximo número de primos para valores consecutivos de $n$, comenzando con $n = 0$, para:\n",
        " - **(a) \\[0.6 puntos\\]** m=1000\n",
        " - **(b) \\[1.6 puntos\\]** m=5000 *(obtener la solución correcta cuenta, pero para alcanzar la puntuación máxima debe aportarse un algoritmo (i) completamente optimizado y (ii) que se ejecute en un tiempo razonable, <~25s)*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pzDsFjtvOrWI",
        "outputId": "6fa37f5d-1c0d-40c6-c387-23cef28fa749"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "129"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TEST 1\n",
        "find_quadratic_prime_coefficients(1000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wA4fpUezOrWJ"
      },
      "outputs": [],
      "source": [
        "# TEST 2\n",
        "find_quadratic_prime_coefficients(5000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XAf2nqEYejpy"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZoqHiLGNOrWJ"
      },
      "source": [
        "----------------\n",
        "\n",
        "## Ejercicio 2 (2.8 puntos)\n",
        "\n",
        "- **(a) \\[0.3 puntos\\]** *Errores de magnitudes derivadas*: Imagina que se han medido dos magnitud $\\tilde{x} = x \\pm E_{a,x}$, e $\\tilde{y} = y \\pm E_{a,y}$ (cada una con un cierto error), y que se quieren utilizar para estimar una nueva variable $\\tilde{z}$, donde $z= z(x, y) = x - y$. ¿Cómo se debe evaluar el error de $z$? ¿Se restan los errores de $x$ y de $y$? ¿Se hace la media? ¿O qué debe hacerse? *Una o dos frases es suficiente*."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JRZEEl1NOrWJ"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2a\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ohnbr9-KQilm"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_aAE_FqOrWJ"
      },
      "source": [
        "\n",
        "---------------------------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CVCM19awOrWJ"
      },
      "source": [
        "El problema de aproximar numéricamente la derivada de una función mediante diferencias finitas puede ser complejo debido a la interacción entre el error de truncamiento (de las series de Taylor) y el error de redondeo (de la representación de coma flotante). En particular, la elección del tamaño de paso $h$ es crítica, ya que afecta a ambos tipos de errores.\n",
        "\n",
        "Sea $f(x)$ una función suave. Supongamos que deseamos calcular su derivada $f'(x)$ en un punto $x_0$. En todo lo que sigue, supóngase un tamaño de paso $h$ constante.\n",
        "\n",
        "- **(b) [0.3 puntos]** Deduce la fórmula de diferencias finitas atrasadas para la primera derivada, de orden 2. Para ello, parte de los desarrollos de Taylor de $f(x_0-h)$ y $f(x_0-2h)$. ¿Qué forma tiene el término del error de truncamiento?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qgs9g7k-OrWJ"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2b\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BXiNRlavOrWK"
      },
      "source": [
        "----------------\n",
        "\n",
        "- **(c) [0.3 puntos]** Supón que los valores de $f(x)$ se calculan con aritmética de coma flotante (es decir, precisión finita), y que el error relativo máximo en $f(x)$ (error de máquina) es $\\epsilon$. En estas condiciones, el error absoluto en $f(x)$ está acotado por $\\epsilon |f(x)|$. Demuestra que el error en la aproximación de diferencias finitas del apartado anterior **debido al redondeo** es\n",
        "\n",
        "$$\n",
        "\\mathcal{O}\\left( \\frac{\\epsilon \\left( |3 f(x_0)| + |4 f(x_0 - h)| + |f(x_0 - 2h)| \\right)}{2 h} \\right)\n",
        "$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vSLLi7yqOrWK"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2c\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C2BL2O3UOrWK"
      },
      "source": [
        "-----\n",
        "\n",
        "- **(d) [0.3 puntos]** Combina el error de truncamiento y el error de redondeo para obtener una única expresión del error total en la aproximación por diferencias finitas para $f'(x_0)$ del apartado (b). Discute cómo depende este error total de $h$ y explica por qué existe un $h$ óptimo que minimiza el error total."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KboTC0LzOrWK"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2d\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7K1R_FM0OrWK"
      },
      "source": [
        "----------------\n",
        "\n",
        " - **(e) [0.4 puntos]**  Para la función $f(x) = \\sin(x)$ en $x_0 = \\frac{\\pi}{4}$, calcula numéricamente la aproximación por diferencias finitas hacia atrás de orden 2 de $f'(x_0)$ usando $h = 10^{-3},\\ 4 \\times 10^{-4},\\ 10^{-4},\\ 4 \\times 10^{-5},\\ 10^{-5},\\ 4 \\times 10^{-6},\\ 10^{-6}$. Registra el error absoluto en cada caso."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PQc5xRzfOrWK"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2e\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "x0 = np.pi / 4  # Punto de evaluación\n",
        "true_derivative = np.cos(x0)  # Valor exacto de la derivada\n",
        "\n",
        "hs = [1e-3, 4e-4, 1e-4, 4e-5, 1e-5, 4e-6, 1e-6]\n",
        "\n",
        "# ... continúa tu solución aquí"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9z-efUJSVaWK"
      },
      "outputs": [],
      "source": [
        "def df1_atrasadas_segundo_orden(h):\n",
        "  '''desarrollo hecho a papel'''\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6eo8yc6OrWK"
      },
      "source": [
        "----------------\n",
        "\n",
        " - **(f) [0.3 puntos]** Grafica el error total (en escala logarítmica) en función de $h$. Identifica el $h$ que minimiza el error total. **Discute el comportamiento observado.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-o3XBLDwOrWK"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2f\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gsAm-R58OrWL"
      },
      "source": [
        "----------------\n",
        "\n",
        "- **(g) [0.9 puntos]** **Extrapolación de Richardson**.\n",
        "\n",
        "La **extrapolación de Richardson** es un método simple para mejorar la precisión de ciertos procedimientos numéricos, incluyendo las aproximaciones por diferencias finita.\n",
        "\n",
        "Supongamos que tenemos una forma de calcular aproximadamente una magnitud $G$, y que el resultado depende de un parámetro $h$. Denotando la aproximación por $g(h)$, podemos escribir $G = g(h) + E_a(h)$, donde $E_a(h)$ representa el error absoluto. La extrapolación de Richardson puede eliminar el error, siempre que este tenga la forma $E_a(h) = c h^p$, siendo $c$ y $p$ constantes.\n",
        "\n",
        "Comenzamos calculando $g(h)$ con algún valor de $h$, digamos $h = h_1$, de modo que:\n",
        "\n",
        "$$\n",
        "G = g(h_1) + c h_1^p\n",
        "$$\n",
        "\n",
        "Luego repetimos el cálculo con $h = h_2$, obteniendo:\n",
        "\n",
        "$$\n",
        "G = g(h_2) + c h_2^p\n",
        "$$\n",
        "\n",
        "Eliminando la constante $c$ y resolviendo para $G$, a partir de las ecuaciones anteriores obtenemos:\n",
        "\n",
        "$$\n",
        "G = \\frac{(h_1/h_2)^p g(h_2) - g(h_1)}{(h_1/h_2)^p - 1},\n",
        "$$\n",
        "\n",
        "que es la fórmula de extrapolación de Richardson. Es común usar $h_2 = h_1/2  = h/2$, en cuyo caso la ecuación anterior se convierte en:\n",
        "\n",
        "$$\n",
        "G = \\frac{2^p g(h/2) - g(h)}{2^p - 1}.\n",
        "$$\n",
        "\n",
        "Aplicando esta técnica a la aproximación anterior por diferencias finitas hacia atrás de $f'(x_0)$, utiliza $h$ y $\\frac{h}{2}$ para obtener una mejor estimación de $f'(x_0)$ que elimine el término de error de orden $\\mathcal{O}(h^2)$.\n",
        "\n",
        "Implementa esta técnica usando los datos del apartado (e). Calcula las aproximaciones mejoradas y registra los errores absolutos. Compara estos errores con los obtenidos en el apartado (e) y discute las mejoras.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxwrmoKIOrWL"
      },
      "outputs": [],
      "source": [
        "# Sol ej 2g"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
