{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ajuste de curvas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducción\n",
    "\n",
    "Si los datos son obtenidos a través de experimentos, generalmente contienen una cantidad significativa de ruido aleatorio causado por errores de medición. La tarea de ajuste de curvas consiste en encontrar una curva suave que se ajuste a los datos \"en promedio\". Esta curva debe tener una forma simple (por ejemplo, un polinomio de bajo orden), para no reproducir el ruido."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Sea\n",
    "$$\n",
    "f(x) = f(x; a_0, a_1, \\dots, a_m)\n",
    "$$\n",
    "la función que debe ajustarse a los $n + 1$ puntos dados $(x_i, y_i)$, $i = 0, 1, \\dots, n$. La notación implica que tenemos una función cuya variable independiente es la $x$ que contiene $m + 1$ parámetros libres a determinar $a_0, a_1, \\dots, a_m$, donde $m < n$. La forma de $f(x)$ se determina de antemano, generalmente a partir de la teoría asociada con el experimento del cual se obtienen los datos. Por ejemplo, si los datos representan los desplazamientos $y_i$ de un sistema masa-muelle en función del tiempo $t$, la teoría sugiere la elección $f(t) = a_0te^{−a_1t}$. \n",
    "\n",
    "Así, el ajuste de curvas consta de dos pasos: \n",
    " 1. Elegir la forma de $f(x)$\n",
    " 2. Calcular de los parámetros que producen el mejor ajuste a los datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Esto nos lleva a la pregunta: ¿Qué se entiende por el \"mejor ajuste\"? Si el ruido de los datos está limitado a la coordenada $y$, el método más utilizado es el **ajuste por mínimos cuadrados**, que minimiza la función de error\n",
    "\n",
    "$$\n",
    "S\\left(a_0, a_1, \\ldots, a_m\\right)=\\sum_{i=0}^n\\left[y_l-f\\left(x_i\\right)\\right]^2\n",
    "$$\n",
    "\n",
    "con respecto a cada $a_j$. Por lo tanto, los valores óptimos de los parámetros se obtienen mediante la solución de las ecuaciones\n",
    "\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial a_k}=0, \\quad k=0,1, \\ldots, m .\n",
    "$$\n",
    "\n",
    "Los términos $r_i = y_i - f(x_i)$ se llaman *residuos*; representan la discrepancia entre los puntos \"dato\" y la función de ajuste evaluada en los $x_i$. La función $S$ que se debe minimizar es, por lo tanto, la suma de los cuadrados de los residuos. El sistema de ecuaciones resultante es generalmente no lineal en los $a_j$ y pueden ser difíciles de resolver. A menudo, la función de ajuste se elige como una combinación lineal de funciones especificadas $f_j(x)$,\n",
    "\n",
    "$$\n",
    "f(x)=a_0 f_0(x)+a_1 f_1(x)+\\cdots+a_m f_m(x)\n",
    "$$\n",
    "\n",
    "en cuyo caso el sistema resultante a resolver es lineal. Si la función de ajuste es un polinomio, tenemos $f_0(x) = 1$, $f_1(x) = x$, $f_2(x) = x^2$, y así sucesivamente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regresión lineal\n",
    "\n",
    "Ajustar una línea recta de la forma\n",
    "\n",
    "$$\n",
    "f(x) = a + bx\n",
    "$$\n",
    "\n",
    "también se conoce como **regresión lineal**. En este caso, la función que se debe minimizar es\n",
    "$$\n",
    "S(a, b)=\\sum_{i=0}^n\\left[y_i-f\\left(x_i\\right)\\right]^2=\\sum_{i=0}^n\\left(y_i-a-b x_i\\right)^2\n",
    "$$\n",
    "El sistema de ecuaciones resultante de derivar la expresión anterior respecto a los parámetros $a$ y $b$ e igualar a $0$ ahora se convierten en:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "& \\frac{\\partial S}{\\partial a}=\\sum_{i=0}^n-2\\left(y_i-a-b x_i\\right)=2\\left[a(n+1)+b \\sum_{i=0}^n x_i-\\sum_{i=0}^n y_i\\right]=0 \\\\\n",
    "& \\frac{\\partial S}{\\partial b}=\\sum_{i=0}^n-2\\left(y_i-a-b x_i\\right) x_i=2\\left(a \\sum_{i=0}^n x_i+b \\sum_{i=0}^n x_i^2-\\sum_{i=0}^n x_i y_i\\right)=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "Dividiendo ambas ecuaciones por $2 (n + 1)$ y reorganizando términos, obtenemos\n",
    "\n",
    "$$\n",
    "a+\\bar{x} b=\\bar{y} \\quad \\bar{x} a+\\left(\\frac{1}{n+1} \\sum_{i=0}^n x_i^2\\right) b=\\frac{1}{n+1} \\sum_{i=0}^n x_i y_i\n",
    "$$\n",
    "donde\n",
    "$$\n",
    "\\bar{x}=\\frac{1}{n+1} \\sum_{i=0}^n x_i \\quad \\bar{y}=\\frac{1}{n+1} \\sum_{i=0}^n y_i\n",
    "$$\n",
    "son las medias de los datos $x$ e $y$. Resolviendo el sistema anterior obtenemos:\n",
    "$$\n",
    "a=\\frac{\\bar{y} \\sum x_i^2-\\bar{x} \\sum x_i y_i}{\\sum x_i^2-n \\bar{x}^2} \\quad b=\\frac{\\sum x_i y_i-\\bar{x} \\sum y_i}{\\sum x_i^2-n \\bar{x}^2}\n",
    "$$\n",
    "Estas expresiones son muy susceptibles a errores de redondeo (los dos términos en cada numerador, así como en cada denominador, pueden ser aproximadamente iguales). Por lo tanto, es mejor calcular los parámetros a partir de las siguientes expresiones:\n",
    "\n",
    "$$\n",
    "b=\\frac{\\sum y_i\\left(x_i-\\bar{x}\\right)}{\\sum x_i\\left(x_i-\\bar{x}\\right)} \\quad a=\\bar{y}-\\bar{x} b\n",
    "$$\n",
    "\n",
    "que se ven mucho menos afectadas por el redondeo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regresión lineal de Ridge \n",
    "\n",
    "Como se verá más adelante, un problema habitual a la hora de plantear un ajuste polinómico es la elección del grado del polinomio, debido a la posibilidad de introducir *overfitting*. Este problema también se presenta cuando nos planteamos cuántas variables independientes introducir en la regresión, en caso de disponer de varias. \n",
    "<center>\n",
    "<div>\n",
    "<img src=\"https://drive.upm.es/index.php/apps/files_sharing/ajax/publicpreview.php?x=1920&y=457&a=true&file=overfit.png&t=bRaBCLDSHGJb55h&scalingup=0\" width=\"400\"/>\n",
    "</div>\n",
    "</center>\n",
    "\n",
    "\n",
    "Una forma habitual de tratar con este problema es introducir una penalización a los términos de mayor orden. En esto se basa la regresión de Ridge. A continuación estudiaremos el caso lineal univariante.\n",
    "\n",
    "En apartados anteriores, el valor a minimizar era la suma al cuadrado de los residuos:\n",
    "$$\n",
    "S(a, b)=\\sum_{i=0}^n\\left(y_i-a-b x_i\\right)^2\n",
    "$$\n",
    "La regresión de Ridge plantea minimizar en su lugar la siguiente cantidad:\n",
    "$$\n",
    "S^*(a, b)=\\sum_{i=0}^n\\left(y_i-a-b x_i\\right)^2 + \\lambda b^2\n",
    "$$\n",
    "donde $\\lambda$ es un factor para controlar la cantidad de penalización al factor $b^2$. El segundo término de este sumando se conoce como *penalización de Ridge*. \n",
    "\n",
    "Si bien la recta resultante no minimiza el error cuadrático medio y por tanto ajusta peor los datos originales, esta nueva linea sufre menos *overfitting* y puede funcionar mejor en predicciones sobre nuevos datos (por ejemplo, cuando se evalúa sobre el *test set* si se utiliza validación cruzada). La idea aquí es *penalizar* el termino lineal, lo que tiende a acercar el ajuste a un polinomio de grado inferior (grado 0 en este caso, *horizontalizando* el ajuste)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Ejercicio 1-** Calcula la recta de regresión lineal para estos datos \n",
    "\n",
    "<center>\n",
    "\n",
    "|   $x$   |   $y$   |\n",
    "|-------|-------|\n",
    "|   1   |   5   |\n",
    "|   3   |   6   |\n",
    "|   4   |   10  |\n",
    "|   5   |   14  |\n",
    "\n",
    "</center>\n",
    "\n",
    "Compárala con la regresión de Ridge ($\\lambda = 14$). Calcula la suma de los residuos al cuadrado en ambos casos. Ahora, supón que se conocen nuevos datos:\n",
    "\n",
    "<center>\n",
    "\n",
    "|   $x$   |   $y$   |\n",
    "|-------|-------|\n",
    "|   2   |  7   |\n",
    "|   5.5   |    11   |\n",
    "\n",
    "</center>\n",
    "\n",
    "\n",
    "\n",
    "¿Cuánto vale la suma de los residuos al cuadrado en estos nuevos datos en cada caso?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.6, 2.2])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#a\n",
    "#desarrollo hechi a papel\n",
    "import numpy as np\n",
    "A = np.array([[4, 13], [13, 51]])\n",
    "B = np.array([35, 133])\n",
    "\n",
    "# Solving the system of equations\n",
    "solution = np.linalg.solve(A, B)\n",
    "solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ajuste de combinaciones lineales de funciones\n",
    "\n",
    "Considere el ajuste de mínimos cuadrados de la siguiente combinación lineal de funciones $f_j(x)$:\n",
    "$$\n",
    "f(x)=a_0 f_0(x)+a_1 f_1(x)+\\ldots+a_m f_m(x)=\\sum_{j=0}^m a_j f_j(x)\n",
    "$$\n",
    "\n",
    "donde cada $f_j (x)$ es una función predeterminada de $x$, llamada a veces *función base*. Sustituyendo en la ecuación del error obtenemos:\n",
    "$$\n",
    "S=\\sum_{i=0}^n\\left[y_i-\\sum_{j=0}^m a_j f_j\\left(x_i\\right)\\right]^2\n",
    "$$\n",
    "Así, las el sistema de ecuaciones resultante de minimizar el error anterior queda:\n",
    "$$\n",
    "\\frac{\\partial S}{\\partial a_k}=-2\\left\\{\\sum_{i=0}^n\\left[y_i-\\sum_{j=0}^m a_j f_j\\left(x_i\\right)\\right] f_k\\left(x_i\\right)\\right\\}=0, \\quad k=0,1, \\ldots, m\n",
    "$$\n",
    "Eliminando la constante (−2) e intercambiando el orden del sumatorio, obtenemos\n",
    "$$\n",
    "\\sum_{j=0}^m\\left[\\sum_{i=0}^n f_j\\left(x_i\\right) f_k\\left(x_i\\right)\\right] a_j=\\sum_{i=0}^n f_k\\left(x_i\\right) y_i, \\quad k=0,1, \\ldots, m\n",
    "$$\n",
    "En notación matricial, estas ecuaciones quedan como\n",
    "$$\n",
    "\\mathbf{A a}=\\mathbf{b}\n",
    "$$\n",
    "donde\n",
    "$$\n",
    "A_{k j}= \\sum_{i=0}^n f_j\\left(x_i\\right) f_k\\left(x_i\\right) \\quad b_k=\\sum_{i=0}^n f_k\\left(x_i\\right) y_i\n",
    "$$\n",
    "Este sistema de ecuaciones se puede resolver con cualquier métodos numérico (se discutirán varios en el siguiente capítulo). Observa que la matriz de coeficientes es simétrica (es decir, $A_{kj} = A_{jk}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ajuste Polinomial\n",
    "\n",
    "Una combinación lineal de funciones comúnmente utilizada es el polinomio. Si el grado del polinomio es $m$, tenemos\n",
    "$$\n",
    "f_j(x)=x^j \\quad(j=0,1, \\ldots, m)\n",
    "$$\n",
    "Aquí, las funciones de base son\n",
    "$$\n",
    "A_{k j}=\\sum_{i=0}^n x_i^{j+k} \\quad b_k=\\sum_{i=0}^n x_i^k y_i \n",
    "$$\n",
    "por lo que el sistema de ecuaciones $\\mathbf{A a}=\\mathbf{b}$ es:\n",
    "$$\n",
    "\\mathbf{A}=\\left[\\begin{array}{lllll}\n",
    "n & \\sum x_i & \\sum x_i^2 & \\ldots & \\sum x_i^m \\\\\n",
    "\\sum x_i & \\sum x_i^2 & \\sum x_i^3 & \\ldots & \\sum x_i^{m+1} \\\\\n",
    "\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "\\sum x_i^{m} & \\sum x_i^{m+1} & \\sum x_i^{m+2} & \\ldots & \\sum x_i^{2 m}\n",
    "\\end{array}\\right] \\quad \\mathbf{b}=\\left[\\begin{array}{l}\n",
    "\\sum y_i \\\\\n",
    "\\sum x_i y_i \\\\\n",
    "\\vdots \\\\\n",
    "\\sum x_i^m y_i\n",
    "\\end{array}\\right]\n",
    "$$\n",
    "donde $\\sum$ representa $\\sum_{i=0}^n$. Este sistema va empeorando su condicionamiento a medida que aumenta $m$. Afortunadamente, esto tiene poco impacto práctico, ya que solo los polinomios de bajo orden son útiles en el ajuste de curvas. No se recomiendan polinomios de alto orden, ya que tienden a reproducir el ruido inherente en los datos (*overfitting*).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 2 -** Crea una función ```poly_fit(x_data, y_data, m)``` que calcule los coeficientes de un polinomio de grado $m$ que ajuste el conjunto de datos ```x_data, y_data```. Puedes utilizar el módulo ```np.linalg``` para resolver el sistema matricial reultante. Comprueba que al ajustar un polinomio de grado 3 con los siguientes datos:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x = np.array([1, 2, 3, 4, 5, 6, 7, 8])\n",
    "y = np.array([1, 3, 2, 4, 3, 6, 7, 9])\n",
    "m = 3\n",
    "x_eval = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "se obtiene una función que evaluada en $x=5$ vale aproximadamente $4.143$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.21428571,  1.34126984, -0.25      ,  0.02777778]),\n",
       " np.float64(4.142857142857157))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "def poly_fit(x_data, y_data, m):\n",
    "    # numer of data. form 0 to n\n",
    "    n = len(x_data) - 1\n",
    "    \n",
    "    # Construc A matix and vector b with m degree\n",
    "    A = np.zeros((m + 1, m + 1))\n",
    "    b = np.zeros(m + 1)\n",
    "    \n",
    "    #fill the matix A with the sums of x_data powers\n",
    "    for j in range(m + 1):\n",
    "        for k in range(m + 1):\n",
    "            A[j, k] = np.sum(x_data ** (j + k))\n",
    "    \n",
    "    #fill the vector b with the sums of y_data multiplied by powers of x_data\n",
    "    for j in range(m + 1):\n",
    "        b[j] = np.sum(y_data * (x_data ** j))\n",
    "    \n",
    "    # solve teh sistem\n",
    "    coefficients = np.linalg.solve(A, b)\n",
    "    return coefficients\n",
    "\n",
    "coefficients = poly_fit(x, y, m)\n",
    "\n",
    "y_eval_corrected = sum(coefficients[i] * (x_eval ** i) for i in range(len(coefficients)))\n",
    "\n",
    "coefficients, y_eval_corrected\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación de la calidad de un ajuste\n",
    "\n",
    "### Coeficiente de determinación $R^2$\n",
    "\n",
    "Una vez obtenido un ajuste (modelo), en ocasiones se desea evaluar la bondad de dicho ajuste; es decir, cómo de bien sirve para explicar los datos.\n",
    "\n",
    "El coeficiente de determinación, a menudo denotado como R-cuadrado ($R^2$), es una medida estadística que representa la proporción de la varianza en la variable dependiente que es predecible a partir de las variables independientes en un modelo de regresión. En otras palabras, mide cómo de bien las variables independientes explican la variabilidad de la variable dependiente. Los valores de $R^2$ van de 0 a 1, siendo valores más altos indicativos de un mejor ajuste del modelo a los datos.\n",
    "\n",
    "$R^2$ puede expresarse en términos de la *sumas total de cuadrados* (TSS) y la *sumas de los cuadrados de los residuos* (RSS). TSS representa la variabilidad total de la variable dependiente, y RSS representa la variabilidad no explicada que permanece después de ajustar el modelo de regresión. Aquí tienes la expresión para R-cuadrado en términos de estos componentes:\n",
    "$$\n",
    "R^2 = 1 - \\frac{RSS}{TSS}\n",
    "$$\n",
    "donde\n",
    " - RSS es la suma de los residuos al cuadrado: $\\sum_{i=1}^n\\left(y_i-\\hat{y}_i\\right)^2$ donde $y_i$ es el valor observado de la variable dependiente y $\\hat{y}_i$ es el valor predicho por el modelo de regresión. \n",
    " - TSS es la\"suma del cuadrado de las distancias de cada punto respecto a la media\": $\\sum_{i=1}^n\\left(y_i-\\bar{y}\\right)^2$ donde $\\bar{y}$ es la media de los valores de la variable dependiente.\n",
    "\n",
    "Un valor más alto de $R^2$ indica que una mayor proporción de la variabilidad en la variable dependiente es explicada por las variables independientes en el modelo.\n",
    "\n",
    "Es importante tener en cuenta que aunque $R^2$ puede proporcionar información sobre la bondad de ajuste de un modelo, tiene limitaciones, especialmente al tratar con modelos complejos o con sobreajuste.\n",
    "\n",
    "\n",
    "### Validación cruzada\n",
    "\n",
    "Para tratar los problemas mencionados anteriormente, una técnica frecuentemente utilizada es la validación cruzada (*cross validation*). Está propone una división del conjunto de datos disponibles en varias partes, generalmente llamadas *folds* (\"pliegues\"). Uno o más *folds* se utilizan como conjunto de prueba para evaluar el rendimiento del modelo, mientras que el resto se utilizan para entrenar el modelo. El proceso puede repetirse varias veces, cada vez con diferentes *folds* como conjuntos de prueba y entrenamiento, y los resultados se promedian para obtener una evaluación más robusta del rendimiento del modelo en diferentes conjuntos de datos.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Ejercicio 3 -** Calcula el coeficiente $R^2$ para las condiciones del Ejercicio 1, con ambas rectas y con ambos datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
