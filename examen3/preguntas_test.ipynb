{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Entendido! Aquí van **100 preguntas tipo verdadero/falso** organizadas por temas. **Primero te doy todas las preguntas** y después, en otra respuesta, te doy las soluciones con justificaciones.\n",
    "\n",
    "---\n",
    "\n",
    "### **Tema 2: Medida del error, representación numérica, errores y estabilidad**  \n",
    "1. El error absoluto mide la diferencia entre el valor aproximado y el valor exacto.  \n",
    "2. El error relativo se calcula como el cociente entre el error absoluto y el valor aproximado.  \n",
    "3. Los errores de redondeo son causados por el uso de operaciones infinitas en métodos numéricos.  \n",
    "4. El error de truncamiento proviene de aproximar procesos infinitos con métodos numéricos finitos.  \n",
    "5. La representación en coma flotante tiene una precisión infinita.  \n",
    "6. La estabilidad numérica de un algoritmo depende del número de iteraciones realizadas.  \n",
    "7. El condicionamiento de un problema mide la sensibilidad de la solución respecto a pequeñas perturbaciones en los datos de entrada.  \n",
    "8. Un problema bien condicionado siempre produce soluciones precisas.  \n",
    "9. Los errores de cancelación catastrófica ocurren al restar números de magnitudes muy cercanas.  \n",
    "10. La precisión de un método numérico puede mejorarse indefinidamente sin límite.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tema 3: Interpolación, ajuste e integración numérica**  \n",
    "**Interpolación y ajuste:**  \n",
    "11. El polinomio interpolante de Lagrange es único para un conjunto dado de puntos.  \n",
    "12. En interpolación, los polinomios de grado alto siempre ofrecen una mejor aproximación.  \n",
    "13. La interpolación de Hermite requiere la continuidad de la derivada en los puntos interpolados.  \n",
    "14. La interpolación lineal usa dos puntos para aproximar una función localmente.  \n",
    "15. En interpolación a trozos, se combinan polinomios de bajo grado.  \n",
    "16. El ajuste por mínimos cuadrados se utiliza cuando los datos contienen ruido.  \n",
    "17. El polinomio de Newton puede calcularse incrementalmente con diferencias divididas.  \n",
    "18. En interpolación cúbica, el polinomio siempre tiene grado 4.  \n",
    "19. La interpolación spline cúbica garantiza continuidad en la segunda derivada.  \n",
    "20. Los métodos de ajuste permiten encontrar funciones que se aproximan a un conjunto de datos.  \n",
    "\n",
    "**Integración numérica:**  \n",
    "21. La regla del trapecio es un método de integración numérica de segundo orden.  \n",
    "22. La regla de Simpson utiliza parábolas para aproximar el área bajo una curva.  \n",
    "23. La integración numérica siempre produce resultados exactos si la función es continua.  \n",
    "24. La precisión de la regla de Simpson es mayor que la de la regla del trapecio.  \n",
    "25. El método de integración de Newton-Cotes se basa en la interpolación polinómica.  \n",
    "26. La cuadratura gaussiana permite obtener una alta precisión con menos puntos.  \n",
    "27. La regla del punto medio tiene error de truncamiento de orden \\( h^2 \\).  \n",
    "28. La integración numérica puede ser inexacta si la función tiene oscilaciones pronunciadas.  \n",
    "29. En la cuadratura compuesta, el intervalo se divide en subintervalos más pequeños.  \n",
    "30. La cuadratura gaussiana requiere una función peso en la integración.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tema 4: Resolución de sistemas lineales**  \n",
    "**Métodos directos:**  \n",
    "31. La eliminación de Gauss es un método directo para resolver sistemas lineales.  \n",
    "32. El método de descomposición LU descompone la matriz en una triangular inferior y superior.  \n",
    "33. El pivotamiento parcial evita problemas de estabilidad numérica.  \n",
    "34. El método LU funciona únicamente para matrices cuadradas.  \n",
    "35. La descomposición de Cholesky solo puede aplicarse a matrices simétricas definidas positivas.  \n",
    "36. El método de Gauss-Jordan es una extensión de la eliminación de Gauss.  \n",
    "37. El condicionamiento de una matriz afecta la precisión en los métodos directos.  \n",
    "38. El pivotamiento completo intercambia filas y columnas para mejorar la estabilidad.  \n",
    "39. El método LU es más eficiente que la eliminación de Gauss para resolver múltiples sistemas con la misma matriz.  \n",
    "40. El tiempo computacional de la eliminación de Gauss es \\( O(n^3) \\) para una matriz \\( n \\times n \\).  \n",
    "\n",
    "**Métodos iterativos:**  \n",
    "41. Los métodos iterativos se usan cuando las matrices son grandes y dispersas.  \n",
    "42. El método de Jacobi requiere que la matriz sea diagonalmente dominante.  \n",
    "43. El método de Gauss-Seidel converge más rápido que el de Jacobi en la mayoría de los casos.  \n",
    "44. La convergencia en los métodos iterativos depende de la matriz del sistema.  \n",
    "45. Los métodos iterativos siempre producen soluciones exactas en un número finito de iteraciones.  \n",
    "46. La convergencia de un método iterativo puede acelerarse utilizando relajación.  \n",
    "47. El método de Sobrelajación Sucesiva (SOR) es una mejora del método de Gauss-Seidel.  \n",
    "48. La diagonal de la matriz influye directamente en la convergencia de los métodos iterativos.  \n",
    "49. Los métodos iterativos permiten resolver sistemas con menor costo computacional que los métodos directos.  \n",
    "50. La matriz identidad es diagonalmente dominante.  \n",
    "\n",
    "---\n",
    "\n",
    "### **Tema 5: Búsqueda de raíces**  \n",
    "51. El método de bisección siempre converge si \\( f(a)f(b) < 0 \\).  \n",
    "52. El método de Newton-Raphson tiene convergencia cuadrática si la derivada no se anula.  \n",
    "53. El método de regula falsi utiliza una interpolación lineal entre dos puntos.  \n",
    "54. El método de bisección es más rápido que Newton-Raphson.  \n",
    "55. En Newton-Raphson, si la derivada es cero, el método falla.  \n",
    "56. El método de la secante no requiere la derivada de la función.  \n",
    "57. La convergencia del método de la secante es superlineal.  \n",
    "58. El método de regula falsi garantiza convergencia incluso si la función es no lineal.  \n",
    "59. El método de bisección divide el intervalo en dos partes iguales en cada iteración.  \n",
    "60. Newton-Raphson puede no converger si el punto inicial está lejos de la raíz.  \n",
    "61. El método de la secante necesita dos valores iniciales.  \n",
    "62. El método de Newton-Raphson requiere que la función sea diferenciable.  \n",
    "63. En el método de la falsa posición, se actualiza siempre el extremo donde cambia el signo de la función.  \n",
    "64. El método de bisección converge de manera lineal.  \n",
    "65. La convergencia cuadrática de Newton-Raphson lo hace más eficiente que la bisección en general.  \n",
    "66. El método de la secante es una variante del método de Newton-Raphson.  \n",
    "67. La regla de Regula Falsi converge más rápido que la bisección en funciones lineales.  \n",
    "68. Si la función tiene múltiples raíces, Newton-Raphson puede saltarse algunas.  \n",
    "69. El método de bisección requiere que la función sea continua en el intervalo dado.  \n",
    "70. El método de Newton-Raphson no necesita un intervalo inicial, solo un punto inicial.  \n",
    "\n",
    "---\n",
    "\n",
    "Estas son **70 preguntas** como adelanto. ¿Te parece bien o continúo hasta completar las **100**?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "¡Perfecto! Aquí tienes las **70 preguntas** de los temas indicados (**verdadero/falso**) con **respuestas y justificaciones**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Respuestas Tema 2: Medida del error, representación numérica, errores y estabilidad**\n",
    "\n",
    "1. **Verdadero**. El error absoluto mide la diferencia entre el valor exacto y el aproximado.  \n",
    "2. **Falso**. El error relativo se calcula como el cociente entre el error absoluto y el **valor exacto** (no el aproximado).  \n",
    "3. **Falso**. Los errores de redondeo surgen al representar números con precisión finita, no por operaciones infinitas.  \n",
    "4. **Verdadero**. El error de truncamiento surge al aproximar métodos numéricos.  \n",
    "5. **Falso**. La representación en coma flotante tiene precisión **limitada**, no infinita.  \n",
    "6. **Falso**. La estabilidad no solo depende del número de iteraciones, sino de la propagación de errores.  \n",
    "7. **Verdadero**. El condicionamiento mide qué tan sensible es la solución a pequeños cambios en los datos.  \n",
    "8. **Falso**. Un problema bien condicionado no garantiza soluciones precisas si se usa un método inestable.  \n",
    "9. **Verdadero**. La cancelación catastrófica ocurre al restar números cercanos en magnitud.  \n",
    "10. **Falso**. Existe un límite práctico para la precisión debido a errores de máquina y truncamiento.\n",
    "\n",
    "---\n",
    "\n",
    "## **Respuestas Tema 3: Interpolación, ajuste e integración numérica**\n",
    "\n",
    "11. **Verdadero**. El polinomio de Lagrange es único para un conjunto de puntos dados.  \n",
    "12. **Falso**. Los polinomios de grado alto pueden oscilar (efecto Runge) y ser menos precisos.  \n",
    "13. **Verdadero**. La interpolación de Hermite garantiza continuidad de la función y su derivada.  \n",
    "14. **Verdadero**. La interpolación lineal aproxima localmente con una recta entre dos puntos.  \n",
    "15. **Verdadero**. La interpolación a trozos usa polinomios de bajo grado en subintervalos.  \n",
    "16. **Verdadero**. El ajuste por mínimos cuadrados es útil cuando los datos tienen ruido.  \n",
    "17. **Verdadero**. El polinomio de Newton usa diferencias divididas de forma incremental.  \n",
    "18. **Falso**. En interpolación cúbica, el polinomio es de **grado 3**, no 4.  \n",
    "19. **Verdadero**. Los splines cúbicos garantizan continuidad hasta la segunda derivada.  \n",
    "20. **Verdadero**. Los métodos de ajuste encuentran funciones que aproximan datos reales.  \n",
    "\n",
    "**Integración numérica:**  \n",
    "21. **Verdadero**. La regla del trapecio tiene error de orden \\( h^2 \\).  \n",
    "22. **Verdadero**. Simpson aproxima con parábolas (grado 2) y es más precisa.  \n",
    "23. **Falso**. La integración numérica no siempre es exacta, depende de la función y método.  \n",
    "24. **Verdadero**. Simpson es más precisa porque tiene un error de orden \\( h^4 \\).  \n",
    "25. **Verdadero**. Newton-Cotes se basa en interpolación polinómica en subintervalos.  \n",
    "26. **Verdadero**. La cuadratura gaussiana logra mayor precisión con menos puntos.  \n",
    "27. **Verdadero**. La regla del punto medio tiene error de orden \\( h^2 \\).  \n",
    "28. **Verdadero**. Las oscilaciones fuertes pueden reducir la precisión de integración numérica.  \n",
    "29. **Verdadero**. La cuadratura compuesta divide el intervalo en subintervalos pequeños.  \n",
    "30. **Verdadero**. La cuadratura gaussiana utiliza una función peso en la integración.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Respuestas Tema 4: Resolución de sistemas lineales**\n",
    "\n",
    "**Métodos directos:**  \n",
    "31. **Verdadero**. La eliminación de Gauss es un método directo para resolver sistemas lineales.  \n",
    "32. **Verdadero**. La descomposición LU descompone una matriz en \\( L \\) y \\( U \\) (inferior y superior).  \n",
    "33. **Verdadero**. El pivotamiento parcial mejora la estabilidad numérica.  \n",
    "34. **Verdadero**. LU solo funciona para matrices cuadradas.  \n",
    "35. **Verdadero**. La descomposición de Cholesky aplica solo a matrices simétricas definidas positivas.  \n",
    "36. **Verdadero**. Gauss-Jordan extiende Gauss para obtener la matriz inversa.  \n",
    "37. **Verdadero**. El condicionamiento afecta la precisión al resolver un sistema.  \n",
    "38. **Verdadero**. El pivotamiento completo intercambia filas y columnas.  \n",
    "39. **Verdadero**. LU es eficiente si se resuelven múltiples sistemas con la misma matriz.  \n",
    "40. **Verdadero**. La eliminación de Gauss tiene complejidad \\( O(n^3) \\).  \n",
    "\n",
    "**Métodos iterativos:**  \n",
    "41. **Verdadero**. Los métodos iterativos son ideales para matrices grandes y dispersas.  \n",
    "42. **Verdadero**. El método de Jacobi requiere que la matriz sea diagonalmente dominante.  \n",
    "43. **Verdadero**. Gauss-Seidel converge más rápido que Jacobi en la mayoría de casos.  \n",
    "44. **Verdadero**. La convergencia depende de las propiedades de la matriz.  \n",
    "45. **Falso**. Los métodos iterativos no siempre producen soluciones exactas.  \n",
    "46. **Verdadero**. La relajación acelera la convergencia en algunos métodos iterativos.  \n",
    "47. **Verdadero**. El método SOR es una versión mejorada de Gauss-Seidel.  \n",
    "48. **Verdadero**. La diagonal influye directamente en la convergencia de iterativos.  \n",
    "49. **Verdadero**. Los métodos iterativos pueden ser más eficientes en sistemas grandes.  \n",
    "50. **Falso**. La matriz identidad no es diagonalmente dominante, pero cumple otras propiedades.  \n",
    "\n",
    "---\n",
    "\n",
    "## **Respuestas Tema 5: Búsqueda de raíces**\n",
    "\n",
    "51. **Verdadero**. La bisección siempre converge si \\( f(a)f(b) < 0 \\).  \n",
    "52. **Verdadero**. Newton-Raphson tiene convergencia cuadrática si \\( f'(x) \\neq 0 \\).  \n",
    "53. **Verdadero**. Regula falsi utiliza interpolación lineal entre dos puntos.  \n",
    "54. **Falso**. Newton-Raphson es más rápido si se cumplen las condiciones.  \n",
    "55. **Verdadero**. Si \\( f'(x) = 0 \\), Newton-Raphson falla.  \n",
    "56. **Verdadero**. La secante no necesita la derivada de la función.  \n",
    "57. **Verdadero**. La convergencia de la secante es superlineal.  \n",
    "58. **Verdadero**. Regula falsi converge incluso en funciones no lineales.  \n",
    "59. **Verdadero**. Bisección divide el intervalo en dos partes iguales.  \n",
    "60. **Verdadero**. Newton-Raphson puede divergir si el punto inicial es lejano.  \n",
    "61. **Verdadero**. La secante necesita dos valores iniciales.  \n",
    "62. **Verdadero**. Newton-Raphson requiere que la función sea diferenciable.  \n",
    "63. **Verdadero**. Regula falsi actualiza el extremo donde cambia el signo.  \n",
    "64. **Verdadero**. La bisección tiene convergencia lineal.  \n",
    "65. **Verdadero**. Newton-Raphson es eficiente por su convergencia cuadrática.  \n",
    "66. **Verdadero**. La secante es una variante de Newton-Raphson.  \n",
    "67. **Verdadero**. Regula falsi converge más rápido que bisección para funciones lineales.  \n",
    "68. **Verdadero**. Newton-Raphson puede \"saltarse\" raíces si hay múltiples raíces.  \n",
    "69. **Verdadero**. La bisección requiere que \\( f(x) \\) sea continua en el intervalo.  \n",
    "70. **Verdadero**. Newton-Raphson solo necesita un punto inicial, no un intervalo.\n",
    "\n",
    "---\n",
    "\n",
    "Si necesitas que continúe hasta las **100 preguntas** o quieres más detalles en algún punto específico, avísame. 😊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
